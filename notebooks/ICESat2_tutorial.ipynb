{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "current-friendship",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": "(function(root) {\n  function now() {\n    return new Date();\n  }\n\n  var force = true;\n  var py_version = '3.2.2'.replace('rc', '-rc.').replace('.dev', '-dev.');\n  var is_dev = py_version.indexOf(\"+\") !== -1 || py_version.indexOf(\"-\") !== -1;\n  var reloading = false;\n  var Bokeh = root.Bokeh;\n  var bokeh_loaded = Bokeh != null && (Bokeh.version === py_version || (Bokeh.versions !== undefined && Bokeh.versions.has(py_version)));\n\n  if (typeof (root._bokeh_timeout) === \"undefined\" || force) {\n    root._bokeh_timeout = Date.now() + 5000;\n    root._bokeh_failed_load = false;\n  }\n\n  function run_callbacks() {\n    try {\n      root._bokeh_onload_callbacks.forEach(function(callback) {\n        if (callback != null)\n          callback();\n      });\n    } finally {\n      delete root._bokeh_onload_callbacks;\n    }\n    console.debug(\"Bokeh: all callbacks have finished\");\n  }\n\n  function load_libs(css_urls, js_urls, js_modules, js_exports, callback) {\n    if (css_urls == null) css_urls = [];\n    if (js_urls == null) js_urls = [];\n    if (js_modules == null) js_modules = [];\n    if (js_exports == null) js_exports = {};\n\n    root._bokeh_onload_callbacks.push(callback);\n\n    if (root._bokeh_is_loading > 0) {\n      console.debug(\"Bokeh: BokehJS is being loaded, scheduling callback at\", now());\n      return null;\n    }\n    if (js_urls.length === 0 && js_modules.length === 0 && Object.keys(js_exports).length === 0) {\n      run_callbacks();\n      return null;\n    }\n    if (!reloading) {\n      console.debug(\"Bokeh: BokehJS not loaded, scheduling load and callback at\", now());\n    }\n\n    function on_load() {\n      root._bokeh_is_loading--;\n      if (root._bokeh_is_loading === 0) {\n        console.debug(\"Bokeh: all BokehJS libraries/stylesheets loaded\");\n        run_callbacks()\n      }\n    }\n    window._bokeh_on_load = on_load\n\n    function on_error() {\n      console.error(\"failed to load \" + url);\n    }\n\n    var skip = [];\n    if (window.requirejs) {\n      window.requirejs.config({'packages': {}, 'paths': {'jspanel': 'https://cdn.jsdelivr.net/npm/jspanel4@4.12.0/dist/jspanel', 'jspanel-modal': 'https://cdn.jsdelivr.net/npm/jspanel4@4.12.0/dist/extensions/modal/jspanel.modal', 'jspanel-tooltip': 'https://cdn.jsdelivr.net/npm/jspanel4@4.12.0/dist/extensions/tooltip/jspanel.tooltip', 'jspanel-hint': 'https://cdn.jsdelivr.net/npm/jspanel4@4.12.0/dist/extensions/hint/jspanel.hint', 'jspanel-layout': 'https://cdn.jsdelivr.net/npm/jspanel4@4.12.0/dist/extensions/layout/jspanel.layout', 'jspanel-contextmenu': 'https://cdn.jsdelivr.net/npm/jspanel4@4.12.0/dist/extensions/contextmenu/jspanel.contextmenu', 'jspanel-dock': 'https://cdn.jsdelivr.net/npm/jspanel4@4.12.0/dist/extensions/dock/jspanel.dock', 'gridstack': 'https://cdn.jsdelivr.net/npm/gridstack@7.2.3/dist/gridstack-all', 'notyf': 'https://cdn.jsdelivr.net/npm/notyf@3/notyf.min'}, 'shim': {'jspanel': {'exports': 'jsPanel'}, 'gridstack': {'exports': 'GridStack'}}});\n      require([\"jspanel\"], function(jsPanel) {\n\twindow.jsPanel = jsPanel\n\ton_load()\n      })\n      require([\"jspanel-modal\"], function() {\n\ton_load()\n      })\n      require([\"jspanel-tooltip\"], function() {\n\ton_load()\n      })\n      require([\"jspanel-hint\"], function() {\n\ton_load()\n      })\n      require([\"jspanel-layout\"], function() {\n\ton_load()\n      })\n      require([\"jspanel-contextmenu\"], function() {\n\ton_load()\n      })\n      require([\"jspanel-dock\"], function() {\n\ton_load()\n      })\n      require([\"gridstack\"], function(GridStack) {\n\twindow.GridStack = GridStack\n\ton_load()\n      })\n      require([\"notyf\"], function() {\n\ton_load()\n      })\n      root._bokeh_is_loading = css_urls.length + 9;\n    } else {\n      root._bokeh_is_loading = css_urls.length + js_urls.length + js_modules.length + Object.keys(js_exports).length;\n    }\n\n    var existing_stylesheets = []\n    var links = document.getElementsByTagName('link')\n    for (var i = 0; i < links.length; i++) {\n      var link = links[i]\n      if (link.href != null) {\n\texisting_stylesheets.push(link.href)\n      }\n    }\n    for (var i = 0; i < css_urls.length; i++) {\n      var url = css_urls[i];\n      if (existing_stylesheets.indexOf(url) !== -1) {\n\ton_load()\n\tcontinue;\n      }\n      const element = document.createElement(\"link\");\n      element.onload = on_load;\n      element.onerror = on_error;\n      element.rel = \"stylesheet\";\n      element.type = \"text/css\";\n      element.href = url;\n      console.debug(\"Bokeh: injecting link tag for BokehJS stylesheet: \", url);\n      document.body.appendChild(element);\n    }    if (((window['jsPanel'] !== undefined) && (!(window['jsPanel'] instanceof HTMLElement))) || window.requirejs) {\n      var urls = ['https://cdn.holoviz.org/panel/1.2.3/dist/bundled/floatpanel/jspanel4@4.12.0/dist/jspanel.js', 'https://cdn.holoviz.org/panel/1.2.3/dist/bundled/floatpanel/jspanel4@4.12.0/dist/extensions/modal/jspanel.modal.js', 'https://cdn.holoviz.org/panel/1.2.3/dist/bundled/floatpanel/jspanel4@4.12.0/dist/extensions/tooltip/jspanel.tooltip.js', 'https://cdn.holoviz.org/panel/1.2.3/dist/bundled/floatpanel/jspanel4@4.12.0/dist/extensions/hint/jspanel.hint.js', 'https://cdn.holoviz.org/panel/1.2.3/dist/bundled/floatpanel/jspanel4@4.12.0/dist/extensions/layout/jspanel.layout.js', 'https://cdn.holoviz.org/panel/1.2.3/dist/bundled/floatpanel/jspanel4@4.12.0/dist/extensions/contextmenu/jspanel.contextmenu.js', 'https://cdn.holoviz.org/panel/1.2.3/dist/bundled/floatpanel/jspanel4@4.12.0/dist/extensions/dock/jspanel.dock.js'];\n      for (var i = 0; i < urls.length; i++) {\n        skip.push(urls[i])\n      }\n    }    if (((window['GridStack'] !== undefined) && (!(window['GridStack'] instanceof HTMLElement))) || window.requirejs) {\n      var urls = ['https://cdn.holoviz.org/panel/1.2.3/dist/bundled/gridstack/gridstack@7.2.3/dist/gridstack-all.js'];\n      for (var i = 0; i < urls.length; i++) {\n        skip.push(urls[i])\n      }\n    }    if (((window['Notyf'] !== undefined) && (!(window['Notyf'] instanceof HTMLElement))) || window.requirejs) {\n      var urls = ['https://cdn.holoviz.org/panel/1.2.3/dist/bundled/notificationarea/notyf@3/notyf.min.js'];\n      for (var i = 0; i < urls.length; i++) {\n        skip.push(urls[i])\n      }\n    }    var existing_scripts = []\n    var scripts = document.getElementsByTagName('script')\n    for (var i = 0; i < scripts.length; i++) {\n      var script = scripts[i]\n      if (script.src != null) {\n\texisting_scripts.push(script.src)\n      }\n    }\n    for (var i = 0; i < js_urls.length; i++) {\n      var url = js_urls[i];\n      if (skip.indexOf(url) !== -1 || existing_scripts.indexOf(url) !== -1) {\n\tif (!window.requirejs) {\n\t  on_load();\n\t}\n\tcontinue;\n      }\n      var element = document.createElement('script');\n      element.onload = on_load;\n      element.onerror = on_error;\n      element.async = false;\n      element.src = url;\n      console.debug(\"Bokeh: injecting script tag for BokehJS library: \", url);\n      document.head.appendChild(element);\n    }\n    for (var i = 0; i < js_modules.length; i++) {\n      var url = js_modules[i];\n      if (skip.indexOf(url) !== -1 || existing_scripts.indexOf(url) !== -1) {\n\tif (!window.requirejs) {\n\t  on_load();\n\t}\n\tcontinue;\n      }\n      var element = document.createElement('script');\n      element.onload = on_load;\n      element.onerror = on_error;\n      element.async = false;\n      element.src = url;\n      element.type = \"module\";\n      console.debug(\"Bokeh: injecting script tag for BokehJS library: \", url);\n      document.head.appendChild(element);\n    }\n    for (const name in js_exports) {\n      var url = js_exports[name];\n      if (skip.indexOf(url) >= 0 || root[name] != null) {\n\tif (!window.requirejs) {\n\t  on_load();\n\t}\n\tcontinue;\n      }\n      var element = document.createElement('script');\n      element.onerror = on_error;\n      element.async = false;\n      element.type = \"module\";\n      console.debug(\"Bokeh: injecting script tag for BokehJS library: \", url);\n      element.textContent = `\n      import ${name} from \"${url}\"\n      window.${name} = ${name}\n      window._bokeh_on_load()\n      `\n      document.head.appendChild(element);\n    }\n    if (!js_urls.length && !js_modules.length) {\n      on_load()\n    }\n  };\n\n  function inject_raw_css(css) {\n    const element = document.createElement(\"style\");\n    element.appendChild(document.createTextNode(css));\n    document.body.appendChild(element);\n  }\n\n  var js_urls = [\"https://cdn.bokeh.org/bokeh/release/bokeh-3.2.2.min.js\", \"https://cdn.bokeh.org/bokeh/release/bokeh-gl-3.2.2.min.js\", \"https://cdn.bokeh.org/bokeh/release/bokeh-widgets-3.2.2.min.js\", \"https://cdn.bokeh.org/bokeh/release/bokeh-tables-3.2.2.min.js\", \"https://cdn.holoviz.org/panel/1.2.3/dist/panel.min.js\"];\n  var js_modules = [];\n  var js_exports = {};\n  var css_urls = [];\n  var inline_js = [    function(Bokeh) {\n      Bokeh.set_log_level(\"info\");\n    },\nfunction(Bokeh) {} // ensure no trailing comma for IE\n  ];\n\n  function run_inline_js() {\n    if ((root.Bokeh !== undefined) || (force === true)) {\n      for (var i = 0; i < inline_js.length; i++) {\n        inline_js[i].call(root, root.Bokeh);\n      }\n      // Cache old bokeh versions\n      if (Bokeh != undefined && !reloading) {\n\tvar NewBokeh = root.Bokeh;\n\tif (Bokeh.versions === undefined) {\n\t  Bokeh.versions = new Map();\n\t}\n\tif (NewBokeh.version !== Bokeh.version) {\n\t  Bokeh.versions.set(NewBokeh.version, NewBokeh)\n\t}\n\troot.Bokeh = Bokeh;\n      }} else if (Date.now() < root._bokeh_timeout) {\n      setTimeout(run_inline_js, 100);\n    } else if (!root._bokeh_failed_load) {\n      console.log(\"Bokeh: BokehJS failed to load within specified timeout.\");\n      root._bokeh_failed_load = true;\n    }\n    root._bokeh_is_initializing = false\n  }\n\n  function load_or_wait() {\n    // Implement a backoff loop that tries to ensure we do not load multiple\n    // versions of Bokeh and its dependencies at the same time.\n    // In recent versions we use the root._bokeh_is_initializing flag\n    // to determine whether there is an ongoing attempt to initialize\n    // bokeh, however for backward compatibility we also try to ensure\n    // that we do not start loading a newer (Panel>=1.0 and Bokeh>3) version\n    // before older versions are fully initialized.\n    if (root._bokeh_is_initializing && Date.now() > root._bokeh_timeout) {\n      root._bokeh_is_initializing = false;\n      root._bokeh_onload_callbacks = undefined;\n      console.log(\"Bokeh: BokehJS was loaded multiple times but one version failed to initialize.\");\n      load_or_wait();\n    } else if (root._bokeh_is_initializing || (typeof root._bokeh_is_initializing === \"undefined\" && root._bokeh_onload_callbacks !== undefined)) {\n      setTimeout(load_or_wait, 100);\n    } else {\n      Bokeh = root.Bokeh;\n      bokeh_loaded = Bokeh != null && (Bokeh.version === py_version || (Bokeh.versions !== undefined && Bokeh.versions.has(py_version)));\n      root._bokeh_is_initializing = true\n      root._bokeh_onload_callbacks = []\n      if (!reloading && (!bokeh_loaded || is_dev)) {\n\troot.Bokeh = undefined;\n      }\n      load_libs(css_urls, js_urls, js_modules, js_exports, function() {\n\tconsole.debug(\"Bokeh: BokehJS plotting callback run at\", now());\n\trun_inline_js();\n      });\n    }\n  }\n  // Give older versions of the autoload script a head-start to ensure\n  // they initialize before we start loading newer version.\n  setTimeout(load_or_wait, 100)\n}(window));",
      "application/vnd.holoviews_load.v0+json": ""
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/javascript": "\nif ((window.PyViz === undefined) || (window.PyViz instanceof HTMLElement)) {\n  window.PyViz = {comms: {}, comm_status:{}, kernels:{}, receivers: {}, plot_index: []}\n}\n\n\n    function JupyterCommManager() {\n    }\n\n    JupyterCommManager.prototype.register_target = function(plot_id, comm_id, msg_handler) {\n      if (window.comm_manager || ((window.Jupyter !== undefined) && (Jupyter.notebook.kernel != null))) {\n        var comm_manager = window.comm_manager || Jupyter.notebook.kernel.comm_manager;\n        comm_manager.register_target(comm_id, function(comm) {\n          comm.on_msg(msg_handler);\n        });\n      } else if ((plot_id in window.PyViz.kernels) && (window.PyViz.kernels[plot_id])) {\n        window.PyViz.kernels[plot_id].registerCommTarget(comm_id, function(comm) {\n          comm.onMsg = msg_handler;\n        });\n      } else if (typeof google != 'undefined' && google.colab.kernel != null) {\n        google.colab.kernel.comms.registerTarget(comm_id, (comm) => {\n          var messages = comm.messages[Symbol.asyncIterator]();\n          function processIteratorResult(result) {\n            var message = result.value;\n            console.log(message)\n            var content = {data: message.data, comm_id};\n            var buffers = []\n            for (var buffer of message.buffers || []) {\n              buffers.push(new DataView(buffer))\n            }\n            var metadata = message.metadata || {};\n            var msg = {content, buffers, metadata}\n            msg_handler(msg);\n            return messages.next().then(processIteratorResult);\n          }\n          return messages.next().then(processIteratorResult);\n        })\n      }\n    }\n\n    JupyterCommManager.prototype.get_client_comm = function(plot_id, comm_id, msg_handler) {\n      if (comm_id in window.PyViz.comms) {\n        return window.PyViz.comms[comm_id];\n      } else if (window.comm_manager || ((window.Jupyter !== undefined) && (Jupyter.notebook.kernel != null))) {\n        var comm_manager = window.comm_manager || Jupyter.notebook.kernel.comm_manager;\n        var comm = comm_manager.new_comm(comm_id, {}, {}, {}, comm_id);\n        if (msg_handler) {\n          comm.on_msg(msg_handler);\n        }\n      } else if ((plot_id in window.PyViz.kernels) && (window.PyViz.kernels[plot_id])) {\n        var comm = window.PyViz.kernels[plot_id].connectToComm(comm_id);\n        comm.open();\n        if (msg_handler) {\n          comm.onMsg = msg_handler;\n        }\n      } else if (typeof google != 'undefined' && google.colab.kernel != null) {\n        var comm_promise = google.colab.kernel.comms.open(comm_id)\n        comm_promise.then((comm) => {\n          window.PyViz.comms[comm_id] = comm;\n          if (msg_handler) {\n            var messages = comm.messages[Symbol.asyncIterator]();\n            function processIteratorResult(result) {\n              var message = result.value;\n              var content = {data: message.data};\n              var metadata = message.metadata || {comm_id};\n              var msg = {content, metadata}\n              msg_handler(msg);\n              return messages.next().then(processIteratorResult);\n            }\n            return messages.next().then(processIteratorResult);\n          }\n        }) \n        var sendClosure = (data, metadata, buffers, disposeOnDone) => {\n          return comm_promise.then((comm) => {\n            comm.send(data, metadata, buffers, disposeOnDone);\n          });\n        };\n        var comm = {\n          send: sendClosure\n        };\n      }\n      window.PyViz.comms[comm_id] = comm;\n      return comm;\n    }\n    window.PyViz.comm_manager = new JupyterCommManager();\n    \n\n\nvar JS_MIME_TYPE = 'application/javascript';\nvar HTML_MIME_TYPE = 'text/html';\nvar EXEC_MIME_TYPE = 'application/vnd.holoviews_exec.v0+json';\nvar CLASS_NAME = 'output';\n\n/**\n * Render data to the DOM node\n */\nfunction render(props, node) {\n  var div = document.createElement(\"div\");\n  var script = document.createElement(\"script\");\n  node.appendChild(div);\n  node.appendChild(script);\n}\n\n/**\n * Handle when a new output is added\n */\nfunction handle_add_output(event, handle) {\n  var output_area = handle.output_area;\n  var output = handle.output;\n  if ((output.data == undefined) || (!output.data.hasOwnProperty(EXEC_MIME_TYPE))) {\n    return\n  }\n  var id = output.metadata[EXEC_MIME_TYPE][\"id\"];\n  var toinsert = output_area.element.find(\".\" + CLASS_NAME.split(' ')[0]);\n  if (id !== undefined) {\n    var nchildren = toinsert.length;\n    var html_node = toinsert[nchildren-1].children[0];\n    html_node.innerHTML = output.data[HTML_MIME_TYPE];\n    var scripts = [];\n    var nodelist = html_node.querySelectorAll(\"script\");\n    for (var i in nodelist) {\n      if (nodelist.hasOwnProperty(i)) {\n        scripts.push(nodelist[i])\n      }\n    }\n\n    scripts.forEach( function (oldScript) {\n      var newScript = document.createElement(\"script\");\n      var attrs = [];\n      var nodemap = oldScript.attributes;\n      for (var j in nodemap) {\n        if (nodemap.hasOwnProperty(j)) {\n          attrs.push(nodemap[j])\n        }\n      }\n      attrs.forEach(function(attr) { newScript.setAttribute(attr.name, attr.value) });\n      newScript.appendChild(document.createTextNode(oldScript.innerHTML));\n      oldScript.parentNode.replaceChild(newScript, oldScript);\n    });\n    if (JS_MIME_TYPE in output.data) {\n      toinsert[nchildren-1].children[1].textContent = output.data[JS_MIME_TYPE];\n    }\n    output_area._hv_plot_id = id;\n    if ((window.Bokeh !== undefined) && (id in Bokeh.index)) {\n      window.PyViz.plot_index[id] = Bokeh.index[id];\n    } else {\n      window.PyViz.plot_index[id] = null;\n    }\n  } else if (output.metadata[EXEC_MIME_TYPE][\"server_id\"] !== undefined) {\n    var bk_div = document.createElement(\"div\");\n    bk_div.innerHTML = output.data[HTML_MIME_TYPE];\n    var script_attrs = bk_div.children[0].attributes;\n    for (var i = 0; i < script_attrs.length; i++) {\n      toinsert[toinsert.length - 1].childNodes[1].setAttribute(script_attrs[i].name, script_attrs[i].value);\n    }\n    // store reference to server id on output_area\n    output_area._bokeh_server_id = output.metadata[EXEC_MIME_TYPE][\"server_id\"];\n  }\n}\n\n/**\n * Handle when an output is cleared or removed\n */\nfunction handle_clear_output(event, handle) {\n  var id = handle.cell.output_area._hv_plot_id;\n  var server_id = handle.cell.output_area._bokeh_server_id;\n  if (((id === undefined) || !(id in PyViz.plot_index)) && (server_id !== undefined)) { return; }\n  var comm = window.PyViz.comm_manager.get_client_comm(\"hv-extension-comm\", \"hv-extension-comm\", function () {});\n  if (server_id !== null) {\n    comm.send({event_type: 'server_delete', 'id': server_id});\n    return;\n  } else if (comm !== null) {\n    comm.send({event_type: 'delete', 'id': id});\n  }\n  delete PyViz.plot_index[id];\n  if ((window.Bokeh !== undefined) & (id in window.Bokeh.index)) {\n    var doc = window.Bokeh.index[id].model.document\n    doc.clear();\n    const i = window.Bokeh.documents.indexOf(doc);\n    if (i > -1) {\n      window.Bokeh.documents.splice(i, 1);\n    }\n  }\n}\n\n/**\n * Handle kernel restart event\n */\nfunction handle_kernel_cleanup(event, handle) {\n  delete PyViz.comms[\"hv-extension-comm\"];\n  window.PyViz.plot_index = {}\n}\n\n/**\n * Handle update_display_data messages\n */\nfunction handle_update_output(event, handle) {\n  handle_clear_output(event, {cell: {output_area: handle.output_area}})\n  handle_add_output(event, handle)\n}\n\nfunction register_renderer(events, OutputArea) {\n  function append_mime(data, metadata, element) {\n    // create a DOM node to render to\n    var toinsert = this.create_output_subarea(\n    metadata,\n    CLASS_NAME,\n    EXEC_MIME_TYPE\n    );\n    this.keyboard_manager.register_events(toinsert);\n    // Render to node\n    var props = {data: data, metadata: metadata[EXEC_MIME_TYPE]};\n    render(props, toinsert[0]);\n    element.append(toinsert);\n    return toinsert\n  }\n\n  events.on('output_added.OutputArea', handle_add_output);\n  events.on('output_updated.OutputArea', handle_update_output);\n  events.on('clear_output.CodeCell', handle_clear_output);\n  events.on('delete.Cell', handle_clear_output);\n  events.on('kernel_ready.Kernel', handle_kernel_cleanup);\n\n  OutputArea.prototype.register_mime_type(EXEC_MIME_TYPE, append_mime, {\n    safe: true,\n    index: 0\n  });\n}\n\nif (window.Jupyter !== undefined) {\n  try {\n    var events = require('base/js/events');\n    var OutputArea = require('notebook/js/outputarea').OutputArea;\n    if (OutputArea.prototype.mime_types().indexOf(EXEC_MIME_TYPE) == -1) {\n      register_renderer(events, OutputArea);\n    }\n  } catch(err) {\n  }\n}\n",
      "application/vnd.holoviews_load.v0+json": ""
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>*[data-root-id],\n",
       "*[data-root-id] > * {\n",
       "  box-sizing: border-box;\n",
       "  font-family: var(--jp-ui-font-family);\n",
       "  font-size: var(--jp-ui-font-size1);\n",
       "  color: var(--vscode-editor-foreground, var(--jp-ui-font-color1));\n",
       "}\n",
       "\n",
       "/* Override VSCode background color */\n",
       ".cell-output-ipywidget-background:has(\n",
       "    > .cell-output-ipywidget-background > .lm-Widget > *[data-root-id]\n",
       "  ),\n",
       ".cell-output-ipywidget-background:has(> .lm-Widget > *[data-root-id]) {\n",
       "  background-color: transparent !important;\n",
       "}\n",
       "</style>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# imports:\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import re\n",
    "import geopandas as gpd\n",
    "import os\n",
    "from IPython.display import Image\n",
    "from IPython.core.display import HTML \n",
    "import hvplot.xarray\n",
    "import pandas as pd\n",
    "import rioxarray"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "together-preparation",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "# N.B.  This notebook is a lot more interesting if initialized with \n",
    "#%matplotlib widget"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "presidential-emerald",
   "metadata": {},
   "source": [
    "# ICESat-2\n",
    "\n",
    "ICESat-2 is a laser altimeter designed to precisely measure the height of snow and ice surfaces using green lasers with small footprints.  Although ICESat-2 doesn't measure surface heights with the same spatial density as airborne laser altimeters, its global spatial coverage makes it a tempting source of free data about snow surfaces.  In this tutorial we will:\n",
    "\n",
    "1. Give a brief overview of ICESat-2\n",
    "\n",
    "2. Show how to find ICESat-2 granues using the IcePyx metadata search tool\n",
    "\n",
    "3. Download some ATL03 photon data from the openAltimetry web service\n",
    "\n",
    "4. Request custom processed height estimates from the SlideRule project.\n",
    "\n",
    "## Measurements and coverage\n",
    "\n",
    "ICESat-2 measures surface heights with six laser beams, grouped into three pairs separated by 3 km, with a 90-m separation between the beams in each pair.\n",
    "\n",
    "Here's a sketch of how this looks (image credit: NSIDC)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "impressive-greeting",
   "metadata": {},
   "outputs": [
    {
     "ename": "HTTPError",
     "evalue": "HTTP Error 404: Not Found",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mHTTPError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m/workspaces/millie-spencer.github.io/notebooks/ICESat2_tutorial.ipynb Cell 4\u001b[0m line \u001b[0;36m1\n\u001b[0;32m----> <a href='vscode-notebook-cell://codespaces%2Bshiny-yodel-vxv9qjpvvpp3xq5r/workspaces/millie-spencer.github.io/notebooks/ICESat2_tutorial.ipynb#W3sdnNjb2RlLXJlbW90ZQ%3D%3D?line=0'>1</a>\u001b[0m Image(\u001b[39m'\u001b[39;49m\u001b[39mhttps://nsidc.org/sites/nsidc.org/files/images/atlas-beam-pattern.png\u001b[39;49m\u001b[39m'\u001b[39;49m, width\u001b[39m=\u001b[39;49m\u001b[39m500\u001b[39;49m)\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/IPython/core/display.py:970\u001b[0m, in \u001b[0;36mImage.__init__\u001b[0;34m(self, data, url, filename, format, embed, width, height, retina, unconfined, metadata, alt)\u001b[0m\n\u001b[1;32m    968\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39munconfined \u001b[39m=\u001b[39m unconfined\n\u001b[1;32m    969\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39malt \u001b[39m=\u001b[39m alt\n\u001b[0;32m--> 970\u001b[0m \u001b[39msuper\u001b[39;49m(Image, \u001b[39mself\u001b[39;49m)\u001b[39m.\u001b[39;49m\u001b[39m__init__\u001b[39;49m(data\u001b[39m=\u001b[39;49mdata, url\u001b[39m=\u001b[39;49murl, filename\u001b[39m=\u001b[39;49mfilename,\n\u001b[1;32m    971\u001b[0m         metadata\u001b[39m=\u001b[39;49mmetadata)\n\u001b[1;32m    973\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mwidth \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmetadata\u001b[39m.\u001b[39mget(\u001b[39m'\u001b[39m\u001b[39mwidth\u001b[39m\u001b[39m'\u001b[39m, {}):\n\u001b[1;32m    974\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mwidth \u001b[39m=\u001b[39m metadata[\u001b[39m'\u001b[39m\u001b[39mwidth\u001b[39m\u001b[39m'\u001b[39m]\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/IPython/core/display.py:327\u001b[0m, in \u001b[0;36mDisplayObject.__init__\u001b[0;34m(self, data, url, filename, metadata)\u001b[0m\n\u001b[1;32m    324\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmetadata \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    325\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmetadata \u001b[39m=\u001b[39m {}\n\u001b[0;32m--> 327\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mreload()\n\u001b[1;32m    328\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_check_data()\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/IPython/core/display.py:1005\u001b[0m, in \u001b[0;36mImage.reload\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1003\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"Reload the raw data from file or URL.\"\"\"\u001b[39;00m\n\u001b[1;32m   1004\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39membed:\n\u001b[0;32m-> 1005\u001b[0m     \u001b[39msuper\u001b[39;49m(Image,\u001b[39mself\u001b[39;49m)\u001b[39m.\u001b[39;49mreload()\n\u001b[1;32m   1006\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mretina:\n\u001b[1;32m   1007\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_retina_shape()\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/IPython/core/display.py:358\u001b[0m, in \u001b[0;36mDisplayObject.reload\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    355\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39murl \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    356\u001b[0m     \u001b[39m# Deferred import\u001b[39;00m\n\u001b[1;32m    357\u001b[0m     \u001b[39mfrom\u001b[39;00m \u001b[39murllib\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mrequest\u001b[39;00m \u001b[39mimport\u001b[39;00m urlopen\n\u001b[0;32m--> 358\u001b[0m     response \u001b[39m=\u001b[39m urlopen(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49murl)\n\u001b[1;32m    359\u001b[0m     data \u001b[39m=\u001b[39m response\u001b[39m.\u001b[39mread()\n\u001b[1;32m    360\u001b[0m     \u001b[39m# extract encoding from header, if there is one:\u001b[39;00m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/urllib/request.py:216\u001b[0m, in \u001b[0;36murlopen\u001b[0;34m(url, data, timeout, cafile, capath, cadefault, context)\u001b[0m\n\u001b[1;32m    214\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    215\u001b[0m     opener \u001b[39m=\u001b[39m _opener\n\u001b[0;32m--> 216\u001b[0m \u001b[39mreturn\u001b[39;00m opener\u001b[39m.\u001b[39;49mopen(url, data, timeout)\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/urllib/request.py:525\u001b[0m, in \u001b[0;36mOpenerDirector.open\u001b[0;34m(self, fullurl, data, timeout)\u001b[0m\n\u001b[1;32m    523\u001b[0m \u001b[39mfor\u001b[39;00m processor \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mprocess_response\u001b[39m.\u001b[39mget(protocol, []):\n\u001b[1;32m    524\u001b[0m     meth \u001b[39m=\u001b[39m \u001b[39mgetattr\u001b[39m(processor, meth_name)\n\u001b[0;32m--> 525\u001b[0m     response \u001b[39m=\u001b[39m meth(req, response)\n\u001b[1;32m    527\u001b[0m \u001b[39mreturn\u001b[39;00m response\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/urllib/request.py:634\u001b[0m, in \u001b[0;36mHTTPErrorProcessor.http_response\u001b[0;34m(self, request, response)\u001b[0m\n\u001b[1;32m    631\u001b[0m \u001b[39m# According to RFC 2616, \"2xx\" code indicates that the client's\u001b[39;00m\n\u001b[1;32m    632\u001b[0m \u001b[39m# request was successfully received, understood, and accepted.\u001b[39;00m\n\u001b[1;32m    633\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39m200\u001b[39m \u001b[39m<\u001b[39m\u001b[39m=\u001b[39m code \u001b[39m<\u001b[39m \u001b[39m300\u001b[39m):\n\u001b[0;32m--> 634\u001b[0m     response \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mparent\u001b[39m.\u001b[39;49merror(\n\u001b[1;32m    635\u001b[0m         \u001b[39m'\u001b[39;49m\u001b[39mhttp\u001b[39;49m\u001b[39m'\u001b[39;49m, request, response, code, msg, hdrs)\n\u001b[1;32m    637\u001b[0m \u001b[39mreturn\u001b[39;00m response\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/urllib/request.py:563\u001b[0m, in \u001b[0;36mOpenerDirector.error\u001b[0;34m(self, proto, *args)\u001b[0m\n\u001b[1;32m    561\u001b[0m \u001b[39mif\u001b[39;00m http_err:\n\u001b[1;32m    562\u001b[0m     args \u001b[39m=\u001b[39m (\u001b[39mdict\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mdefault\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mhttp_error_default\u001b[39m\u001b[39m'\u001b[39m) \u001b[39m+\u001b[39m orig_args\n\u001b[0;32m--> 563\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call_chain(\u001b[39m*\u001b[39;49margs)\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/urllib/request.py:496\u001b[0m, in \u001b[0;36mOpenerDirector._call_chain\u001b[0;34m(self, chain, kind, meth_name, *args)\u001b[0m\n\u001b[1;32m    494\u001b[0m \u001b[39mfor\u001b[39;00m handler \u001b[39min\u001b[39;00m handlers:\n\u001b[1;32m    495\u001b[0m     func \u001b[39m=\u001b[39m \u001b[39mgetattr\u001b[39m(handler, meth_name)\n\u001b[0;32m--> 496\u001b[0m     result \u001b[39m=\u001b[39m func(\u001b[39m*\u001b[39;49margs)\n\u001b[1;32m    497\u001b[0m     \u001b[39mif\u001b[39;00m result \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    498\u001b[0m         \u001b[39mreturn\u001b[39;00m result\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/urllib/request.py:643\u001b[0m, in \u001b[0;36mHTTPDefaultErrorHandler.http_error_default\u001b[0;34m(self, req, fp, code, msg, hdrs)\u001b[0m\n\u001b[1;32m    642\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mhttp_error_default\u001b[39m(\u001b[39mself\u001b[39m, req, fp, code, msg, hdrs):\n\u001b[0;32m--> 643\u001b[0m     \u001b[39mraise\u001b[39;00m HTTPError(req\u001b[39m.\u001b[39mfull_url, code, msg, hdrs, fp)\n",
      "\u001b[0;31mHTTPError\u001b[0m: HTTP Error 404: Not Found"
     ]
    }
   ],
   "source": [
    "Image('https://nsidc.org/sites/nsidc.org/files/images/atlas-beam-pattern.png', width=500)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "critical-queens",
   "metadata": {},
   "source": [
    "ICESat-2 flies a repeat orbit with 1387 ground tracks every 91 days, but over Grand Mesa, the collection strategy (up until now) has designed to optimize spatial coverage, so the measurements are shifted to the left and right of the repeat tracks to help densify the dataset.  We should expect to see tracks running (approximately) north-south over the Mesa, in tripplets of pairs that are scattered from east to west.   Because clouds often block the laser, not every track will return usable data.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "annual-bottom",
   "metadata": {},
   "outputs": [],
   "source": [
    "Image('https://nsidc.org/sites/nsidc.org/files/images/icesat-2-spots-beams-fwd-rev.png', width=500)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "julian-enterprise",
   "metadata": {},
   "source": [
    "We describe ICESat-2's beam layout on the ground based on pairs (numbered 1, 2, and 3, from left to right) and the location of each beam in each pair (L, R).  Thus GT2L is the left beam in the center pair.  In each pair, one beam is always stronger than the other (to help penetrate thin clouds), but since the spacecraft sometimes reverses its orientation to keep the solar panels illuminated, the strong beam can be either left or right, depending on the phase of the mission.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "tired-france",
   "metadata": {},
   "source": [
    "## Basemap (Sentinel)\n",
    "\n",
    "To get a sense of where the data are, we're going to use an Sentinel SAR image of Grand Mesa.  I've stolen this snippet of code from the SAR tutorial:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "indoor-interference",
   "metadata": {},
   "outputs": [],
   "source": [
    "# GDAL environment variables to efficiently read remote data\n",
    "os.environ['GDAL_DISABLE_READDIR_ON_OPEN']='EMPTY_DIR' \n",
    "os.environ['AWS_NO_SIGN_REQUEST']='YES' \n",
    "\n",
    "# SAR Data are stored in a public S3 Bucket\n",
    "url = 's3://sentinel-s1-rtc-indigo/tiles/RTC/1/IW/12/S/YJ/2016/S1B_20161121_12SYJ_ASC/Gamma0_VV.tif'\n",
    "\n",
    "# These Cloud-Optimized-Geotiff (COG) files have 'overviews', low-resolution copies for quick visualization\n",
    "XR=[725000.0, 767000.0]\n",
    "YR=[4.30e6, 4.34e6]\n",
    "# open the dataset\n",
    "da = rioxarray.open_rasterio(url, overview_level=1).squeeze('band')#.clip_box([712410.0, 4295090.0, 797010.0, 4344370.0])\n",
    "da=da.where((da.x>XR[0]) & (da.x < XR[1]), drop=True)\n",
    "da=da.where((da.y>YR[0]) & (da.y < YR[1]), drop=True)\n",
    "dx=da.x[1]-da.x[0]\n",
    "SAR_extent=[da.x[0]-dx/2, da.x[-1]+dx/2, np.min(da.y)-dx/2, np.max(da.y)+dx/2]\n",
    "\n",
    "# Prepare coordinate transformations into the basemap coordinate system\n",
    "from pyproj import Transformer, CRS\n",
    "crs=CRS.from_wkt(da['spatial_ref'].spatial_ref.crs_wkt)\n",
    "to_image_crs=Transformer.from_crs(crs.geodetic_crs, crs)\n",
    "to_geo_crs=Transformer.from_crs(crs, crs.geodetic_crs)\n",
    "\n",
    "corners_lon, corners_lat=to_geo_crs.transform(np.array(XR)[[0, 1, 1, 0, 0]], np.array(YR)[[0, 0, 1, 1, 0]])\n",
    "lonlims=[np.min(corners_lat), np.max(corners_lat)]\n",
    "latlims=[np.min(corners_lon), np.max(corners_lon)]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "liable-bradford",
   "metadata": {},
   "source": [
    "## Searching for ICESat-2 data using IcePyx\n",
    "\n",
    "The IcePyx library has functions for searching for ICEsat-2 data, as well as subsetting it and retrieving it from NSIDC.  We're going to use the search functions today, because we don't need to retrieve the complete ICESat-2 products."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "complicated-reach",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import icepyx as ipx\n",
    "\n",
    "region_a = ipx.Query('ATL03', [lonlims[0], latlims[0], lonlims[1], latlims[1]], ['2018-12-01','2021-06-01'], \\\n",
    "                          start_time='00:00:00', end_time='23:59:59')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abroad-bracket",
   "metadata": {},
   "source": [
    "To run this next section, you'll need to setup your netrc file to connect to nasa earthdata.  During the hackweek we will use machine credentials, but afterwards, you may need to use your own credentials.  The login procedure is in the next cell, commented out."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "utility-growth",
   "metadata": {},
   "outputs": [],
   "source": [
    "#earthdata_uid = 'your_name_here'\n",
    "#email = 'your@email'\n",
    "#region_a.earthdata_login(earthdata_uid, email)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "scientific-handle",
   "metadata": {},
   "source": [
    "Once we're logged in, the avail_granules() fetches a list of available ATL03 granules:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "growing-trick",
   "metadata": {},
   "outputs": [],
   "source": [
    "region_a.avail_granules()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cultural-order",
   "metadata": {},
   "source": [
    "The filename for each granule (which contains lots of handy information) is in the 'producer_granule_id' field: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "divine-degree",
   "metadata": {},
   "outputs": [],
   "source": [
    "region_a.granules.avail[0]['producer_granule_id']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "charged-probe",
   "metadata": {},
   "source": [
    "The filename contains ATL03_YYYYMMDDHHMMSS_TTTTCCRR_rrr_vv.h5 where:\n",
    "\n",
    " * YYYMMDDHHMMSS gives the date (to the second) of the start of the granule\n",
    " * TTTT gives the ground-track number\n",
    " * CC gives the cycle number \n",
    " * RR gives the region (what part of the orbit this is) \n",
    " * rrr_vv give the release and version\n",
    " \n",
    " Let's strip out the date using a regular expression, and see when ICESat-2 flew over Grand Mesa:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "inside-delicious",
   "metadata": {},
   "outputs": [],
   "source": [
    "ATLAS_re=re.compile('ATL.._(?P<year>\\d\\d\\d\\d)(?P<month>\\d\\d)(?P<day>\\d\\d)\\d+_(?P<track>\\d\\d\\d\\d)')\n",
    "\n",
    "date_track=[]\n",
    "for count, item in enumerate(region_a.granules.avail):\n",
    "    granule_info=ATLAS_re.search(item['producer_granule_id']).groupdict()\n",
    "    date_track += [ ('-'.join([granule_info[key] for key in ['year', 'month', 'day']]), granule_info['track'])]\n",
    "\n",
    "# print the first ten dates and ground tracks, plus their indexes\n",
    "[(count, dt) for count, dt in enumerate(date_track[0:10])]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "prepared-engineer",
   "metadata": {},
   "source": [
    "From this point, the very capable icepyx interface allows you to order either full data granules or subsets of granules from NSIDC.  Further details are available from https://icepyx.readthedocs.io/en/latest/, and their 'examples' pages are quite helpful.  Note that ATL03 photon data granules are somewhat cumbersome, so downloading them without subsetting will be time consuming, and requesting subsetting from NSIDC may take a while.  \n",
    "\n",
    "## Ordering photon data from openAltimetry\n",
    "For ordering small numbers of points (up to one degree worth of data), the openAltimetry service provides very quick and efficient access to a simplified version of the ATL03 data.  Their API (https://openaltimetry.org/data/swagger-ui/) allows us to build web queries for the data.  We'll use that for a quick look at the data over Grand Mesa, initially reading just one central beam pair:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "guided-enough",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_OA(date_track, lonlims, latlims, beamnames=[\"gt1l\",\"gt1r\",\"gt2l\",\"gt2r\",\"gt3l\",\"gt3r\"]):\n",
    "    '''\n",
    "    retrieve ICESat2 ATL03 data from openAltimetry\n",
    "    \n",
    "    Inputs:\n",
    "        date_track: a list of tuples.  Each contains a date string \"YYYY-MM-DD\" and track number (4-character string)\n",
    "        lonlims: longitude limits for the search\n",
    "        latlims: latitude limits for the search\n",
    "        beamnames: list of strings for the beams\n",
    "    outputs:\n",
    "        a dict containing ATL03 data by beam name\n",
    "    \n",
    "    Due credit:\n",
    "        Much of this code was borrowed Philipp Arndt's Pond Picker repo: https://github.com/fliphilipp/pondpicking\n",
    "    '''\n",
    "    \n",
    "    \n",
    "    IS2_data={}\n",
    "    for this_dt in date_track:\n",
    "        this_IS2_data={}\n",
    "        for beamname in beamnames:\n",
    "            oa_url = 'https://openaltimetry.org/data/api/icesat2/atl03?minx={minx}&miny={miny}&maxx={maxx}&maxy={maxy}&trackId={trackid}&beamName={beamname}&outputFormat=json&date={date}&client=jupyter'\n",
    "            oa_url = oa_url.format(minx=lonlims[0],miny=latlims[0],maxx=lonlims[1], maxy=latlims[1], \n",
    "                                   trackid=this_dt[1], beamname=beamname, date=this_dt[0], sampling='true')\n",
    "            #.conf_ph = ['Noise','Buffer', 'Low', 'Medium', 'High']\n",
    "            if True:\n",
    "                r = requests.get(oa_url)\n",
    "                data = r.json()\n",
    "                D={}\n",
    "                D['lat_ph'] = []\n",
    "                D['lon_ph'] = []\n",
    "                D['h_ph'] = []\n",
    "                D['conf_ph']=[]\n",
    "                conf_ph = {'Noise':0, 'Buffer':1, 'Low':2, 'Medium':3, 'High':4}\n",
    "                for beam in data:\n",
    "                    for photons in beam['series']:\n",
    "                        for conf, conf_num in conf_ph.items():         \n",
    "                            if conf in photons['name']:\n",
    "                                for p in photons['data']:\n",
    "                                    \n",
    "                                    D['lat_ph'].append(p[0])\n",
    "                                    D['lon_ph'].append(p[1])\n",
    "                                    D['h_ph'].append(p[2])\n",
    "                                    D['conf_ph'].append(conf_num)\n",
    "                    D['x_ph'], D['y_ph']=to_image_crs.transform(D['lat_ph'], D['lon_ph'])\n",
    "                for key in D:\n",
    "                    D[key]=np.array(D[key])\n",
    "                if len(D['lat_ph']) > 0:\n",
    "                    this_IS2_data[beamname]=D\n",
    "            #except Exception as e:\n",
    "            #    print(e)\n",
    "            #    pass\n",
    "        if len(this_IS2_data.keys()) > 0:\n",
    "            IS2_data[this_dt] = this_IS2_data\n",
    "    return IS2_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "rocky-showcase",
   "metadata": {},
   "outputs": [],
   "source": [
    "#submitting all of these requests should take about 1 minute\n",
    "IS2_data=get_OA(date_track, lonlims, latlims, ['gt2l'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ambient-thumb",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "plt.imshow(np.array(da)[::-1,:], origin='lower', extent=SAR_extent, cmap='gray', clim=[0, 0.5])#plt.figure();\n",
    "\n",
    "for dt, day_data in IS2_data.items():\n",
    "    for beam, D in day_data.items():\n",
    "        plt.plot(D['x_ph'][::10], D['y_ph'][::10], '.', markersize=3, label=str(dt))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "affected-grass",
   "metadata": {},
   "source": [
    "What we see in this plot is Grand Mesa, with lines showing data from the center beams of several tracks passing across it.  A few of these tracks have been repeated, but most are offset from the others.  Looking at these, it should be clear that the quality of the data is not consistent from track to track.  Some are nearly continuous, others have gaps, and other still have no data at all and are not plotted here.  Remember, though, that what we've plotted here are just the center beams.  There are a total of two more beam pairs, and a total of five more beams!\n",
    "\n",
    "To get an idea of what the data look like, we'll pick one of the tracks and plot its elevation profile.  In interactive mode (%matplotlib widget) it's possible to zoom in on the plot, query the x and y limits, and use these to identify the data for the track that intersects an area of interest.  I've done this to pick two good-looking tracks, but you can uncomment the first two lines here and zoom in yourself to look at other tracks:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "composite-holly",
   "metadata": {},
   "outputs": [],
   "source": [
    "XR=plt.gca().get_xlim()\n",
    "YR=plt.gca().get_ylim()\n",
    "print(XR)\n",
    "print(YR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "macro-shannon",
   "metadata": {},
   "outputs": [],
   "source": [
    "#XR=plt.gca().get_xlim()\n",
    "#YR=plt.gca().get_ylim()\n",
    "XR=(740773.7483556366, 741177.9430390946)\n",
    "YR=(4325197.508090873, 4325728.013612912)\n",
    "\n",
    "dts_in_axes=[]\n",
    "for dt, day_data in IS2_data.items():\n",
    "    for beam, D in day_data.items():\n",
    "        if np.any(\n",
    "            (D['x_ph'] > XR[0]) & (D['x_ph'] < XR[1]) &\n",
    "            (D['y_ph'] > np.min(YR)) & (D['y_ph'] < np.max(YR))):\n",
    "            dts_in_axes += [dt]\n",
    "dts_in_axes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "alike-blake",
   "metadata": {},
   "source": [
    "Based on the axis limits I filled in, Track 295 has two repeats over the mesa that nearly coincide.\n",
    "\n",
    "Now we can get the full (six-beam) dataset for one of these repeats and plot it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "animated-screening",
   "metadata": {},
   "outputs": [],
   "source": [
    "full_track_data=get_OA([dts_in_axes[0]], lonlims, latlims)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "extra-salem",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "fig=plt.figure(); \n",
    "hax=fig.subplots(1, 2)\n",
    "plt.sca(hax[0])\n",
    "plt.imshow(np.array(da)[::-1,:], origin='lower', extent=SAR_extent, cmap='gray', clim=[0, 0.5])#plt.figure();\n",
    "\n",
    "for dt, day_data in full_track_data.items():\n",
    "    for beam, D in day_data.items():\n",
    "        plt.plot(D['x_ph'], D['y_ph'],'.', markersize=1)\n",
    "plt.title(dts_in_axes[0])\n",
    "\n",
    "plt.sca(hax[1])\n",
    "D=day_data['gt2l']\n",
    "colors_key={((0,1)):'k', (2,3,4):'r'}\n",
    "for confs, color in colors_key.items():\n",
    "    for conf in confs:\n",
    "        these=np.flatnonzero(D['conf_ph']==conf)\n",
    "        plt.plot(D['y_ph'][these], D['h_ph'][these],'.', color=color, markersize=1)#label=','.join(list(confs)))\n",
    "plt.ylabel('WGS-84 height, m');\n",
    "plt.xlabel('UTM-12 northing, m');\n",
    "plt.title('gt2l');\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "received-strap",
   "metadata": {},
   "source": [
    "On the left we see a plot of all six beams crossing (or almost crossing) Grand Mesa, in April of 2020.  If you zoom in on the plot, you can distinguish the beam pairs into separate beams.  On the right, we see one of the central beams crossing the mesa from south to north.  There is a broad band of noise photons that were close enough to the ground to be telemetered by the satellite, and a much narrower band (in red) of photons identified by the processing software as likely coming from the ground."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "piano-annex",
   "metadata": {},
   "source": [
    "These data give a maximum of detail about what the surface looks like to ICESat-2.  to reduce this to elevation data, telling the surface height at specific locations, there are a few options:\n",
    "    \n",
    "    1. Download higher-level products (i.e. ATL06, ATL08) from NSIDC\n",
    "    2. Calculate statistics of the photons (i.e. a running mean of the flagged photon heights\n",
    "    3. Ask the SlideRule service to calculate along-track averages of the photon heights.\n",
    "\n",
    "We're going to try (3)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "automotive-alberta",
   "metadata": {},
   "source": [
    "## Ordering surface-height segments from SlideRule\n",
    "\n",
    "SildeRule is a new and exciting (to me) system that does real-time processing of ICESat-2 data _in the cloud_ while also offering efficient web-based delivery of data products.  It's new, and it's not available for all locations, but Grand Mesa is one of the test sites, so we should be able to get access to the full set of ATL03 data there.\n",
    "[MORE WORK TO GO HERE]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "prime-vermont",
   "metadata": {},
   "source": [
    "You'll need to install the sliderule-python package, available from https://github.com/ICESat2-SlideRule/sliderule-python\n",
    "This package has been installed on the hub, but if you need it, these commands will install it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "federal-aside",
   "metadata": {},
   "outputs": [],
   "source": [
    "#! [ -d sliderule-python ] || git clone https://github.com/ICESat2-SlideRule/sliderule-python.git \n",
    "#! cd sliderule-python; python setup.py develop"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "approved-edgar",
   "metadata": {},
   "source": [
    "We will submit a query to sliderule to process all of the data that CMR finds for our region, fitting 20-meter line-segments to all of the photons with medium-or-better signal confidence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "turned-google",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sliderule import icesat2\n",
    "\n",
    "# initialize\n",
    "icesat2.init(\"icesat2sliderule.org\", verbose=False)\n",
    "\n",
    "# region of interest polygon\n",
    "region = [ {\"lon\":lon_i, \"lat\":lat_i} for lon_i, lat_i in \n",
    "          zip(np.array(lonlims)[[0, -1, -1, 0, 0]],  np.array(latlims)[[0, 0, -1, -1, 0]])]\n",
    "\n",
    "# request parameters\n",
    "params = {\n",
    "    \"poly\": region,  # request the polygon defined by our lat-lon bounds\n",
    "    \"srt\": icesat2.SRT_LAND, # request classification based on the land algorithm\n",
    "    \"cnf\": icesat2.CNF_SURFACE_MEDIUM, # use all photons of low confidence or better\n",
    "    \"len\": 20.0,  # fit data in overlapping 40-meter segments\n",
    "    \"res\": 10.0,  # report one height every 20 m\n",
    "    \"ats\":5., #report a segment only if it contains at least 2 photons separated by 5 m\n",
    "    \"maxi\": 6,  # allow up to six iterations in fitting each segment to the data\n",
    "}\n",
    "\n",
    "# make request\n",
    "rsps = icesat2.atl06p(params, \"atlas-s3\")\n",
    "\n",
    "# save the result in a dataframe\n",
    "df = pd.DataFrame(rsps)\n",
    "\n",
    "# calculate the polar-stereographic coordinates:\n",
    "df['x'], df['y']=to_image_crs.transform(df['lat'], df['lon'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "reported-hazard",
   "metadata": {},
   "source": [
    "SlideRule complains when it tries to calculate heights within our ROI for ground tracks that don't intersect the ROI.  This happens quite a bit because the CMR service that IcePyx and SlideRule use to search for the data uses a generous buffer on each ICESat-2 track.  It shouldn't bother us.  In fact, we have quite a few tracks for our region.\n",
    "\n",
    "Let's find all the segments from rgt 295, cycle 7 and map their heights:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "senior-shepherd",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(); \n",
    "plt.imshow(np.array(da)[::-1,:], origin='lower', extent=SAR_extent, cmap='gray', clim=[0, 0.5])#plt.figure();\n",
    "ii=(df['rgt']==295) & (df['cycle']==7)\n",
    "plt.scatter(df['x'][ii], df['y'][ii],4, c=df['h_mean'][ii], cmap='gist_earth')\n",
    "plt.colorbar()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "technical-tracker",
   "metadata": {},
   "source": [
    "As we saw a few cells up, for track 295 cycles 7 and 8 are nearly exact repeats.  Cycle 7 was April 2020, cycle 8 was July 2020.  Could it be that we can measure snow depth in April by comparing the two?  Let's plot spot 3 for both!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "precise-column",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure();\n",
    "ii=(df['rgt']==295) & (df['cycle']==7) & (df['spot']==3)\n",
    "plt.plot(df['y'][ii], df['h_mean'][ii],'.', label='April')\n",
    "ii=(df['rgt']==295) & (df['cycle']==8) & (df['spot']==3)\n",
    "plt.plot(df['y'][ii], df['h_mean'][ii],'.', label='July')\n",
    "\n",
    "plt.legend()\n",
    "\n",
    "plt.xlabel('polar stereographic northing, m')\n",
    "plt.ylabel('height, m')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "unusual-merchandise",
   "metadata": {},
   "source": [
    "To try to get at snow depth, we can look for bare-earth DTMs here:\n",
    "    'https://prd-tnm.s3.amazonaws.com/LidarExplorer/index.html#'\n",
    "I've picked one of the 1-meter DTMs that covers part of track 295.  We'll read it directly from s3 with the rasterio/xarray package, and downsample it to 3m (to save time later)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "persistent-click",
   "metadata": {},
   "outputs": [],
   "source": [
    "import rioxarray as rxr\n",
    "from rasterio.enums import Resampling\n",
    "\n",
    "url='https://prd-tnm.s3.amazonaws.com/StagedProducts/Elevation/1m/Projects/CO_MesaCo_QL2_UTM12_2016/TIFF/USGS_one_meter_x74y433_CO_MesaCo_QL2_UTM12_2016.tif'\n",
    "\n",
    "lidar_ds=rxr.open_rasterio(url)\n",
    "#resample the DTM to ~3m:\n",
    "scale_factor = 1/3\n",
    "new_width = int(lidar_ds.rio.width * scale_factor)\n",
    "new_height = int(lidar_ds.rio.height * scale_factor)\n",
    "\n",
    "#reproject the horizontal CRS to match ICESat-2\n",
    "UTM_wgs84_crs=CRS.from_epsg(32612)\n",
    "lidar_3m = lidar_ds.rio.reproject(\n",
    "    UTM_wgs84_crs,\n",
    "    shape=(new_height, new_width),\n",
    "    resampling=Resampling.bilinear,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "official-monitor",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(); \n",
    "lidar_3m.sel(band=1).plot.imshow()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "statutory-emergency",
   "metadata": {},
   "source": [
    "To compare the DTM directly with the ICESat-2 data, we'll need to sample it at the ICESat-2 points.  There are probably ways to do this directly in xarray, but I'm not an expert.  Here we'll use a scipy interpolator:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fatal-tribute",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.interpolate import RectBivariateSpline\n",
    "interpolator = RectBivariateSpline(np.array(lidar_3m.y)[::-1], np.array(lidar_3m.x), \n",
    "                                   np.array(lidar_3m.sel(band=1))[::-1,:], kx=1, ky=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "emerging-jamaica",
   "metadata": {},
   "outputs": [],
   "source": [
    "x0=np.array(lidar_3m.x)\n",
    "y0=np.array(lidar_3m.y)\n",
    "\n",
    "ii=(df['rgt']==295) & (df['cycle']==7) & (df['spot']==3)\n",
    "ii &= (df['x'] > np.min(x0)) & (df['x'] < np.max(x0))\n",
    "ii &= (df['y'] > np.min(y0)) & (df['y'] < np.max(y0))\n",
    "\n",
    "zi=interpolator.ev(df['y'][ii], df['x'][ii])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "personalized-filling",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig=plt.figure(figsize=[8, 5]); \n",
    "hax=fig.subplots(1,2)\n",
    "plt.sca(hax[0])\n",
    "lidar_3m.sel(band=1).plot.imshow()\n",
    "plt.plot(df['x'][ii], df['y'][ii],'.')\n",
    "plt.axis('equal')\n",
    "\n",
    "plt.sca(hax[1])\n",
    "plt.plot(df['y'][ii], df['h_mean'][ii],'.', label='April')\n",
    "plt.plot(df['y'][ii], zi,'.', label='DTM')\n",
    "plt.legend()\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "effective-resource",
   "metadata": {},
   "source": [
    "The DTM is below the April ICESat-2 heights.  That's probably not right, and it's because we don't have the vertical datums correct here (ICESat-2 WGS84, the DEM is NAD83).  That's OK!  Since we have multiple passes over the same DEM, we can use the DEM to correct for spatial offsets between the measurements.  Let's use the DEM to correct for differences between the July and April data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aboriginal-possible",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "\n",
    "ii=(df['rgt']==295) & (df['cycle']==7) & (df['spot']==3)\n",
    "ii &= (df['x'] > np.min(x0)) & (df['x'] < np.max(x0))\n",
    "ii &= (df['y'] > np.min(y0)) & (df['y'] < np.max(y0))\n",
    "zi=interpolator.ev(df['y'][ii], df['x'][ii])\n",
    "\n",
    "plt.plot(df['y'][ii], df['h_mean'][ii]-zi,'.', label='April')\n",
    "\n",
    "ii=(df['rgt']==295) & (df['cycle']==8) & (df['spot']==3)\n",
    "ii &= (df['x'] > np.min(x0)) & (df['x'] < np.max(x0))\n",
    "ii &= (df['y'] > np.min(y0)) & (df['y'] < np.max(y0))\n",
    "zi=interpolator.ev(df['y'][ii], df['x'][ii])\n",
    "\n",
    "\n",
    "plt.plot(df['y'][ii], df['h_mean'][ii]-zi,'.', label='July')\n",
    "plt.gca().set_ylim([-20, -10])\n",
    "\n",
    "plt.legend()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "foreign-auction",
   "metadata": {},
   "source": [
    "This looks good, if a little noisy.  We could get a better comparison by (1) using multiple ICESat-2 tracks to extract a mean snow-off difference between the DTM and ICESat-2, or (2). finding adjacent pairs of measurements  between the two tracks, and comparing their heights directly.  These are both good goals for projects!\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "naval-discount",
   "metadata": {},
   "source": [
    "## Further reading:\n",
    "\n",
    "\n",
    "There are lots of resources available for ICESat-2 data on the web.  Two of the best are the NSIDC ICESat-2 pages:\n",
    "\n",
    "https://nsidc.org/data/icesat-2\n",
    "\n",
    "and NASA's ICESat-2 page:\n",
    "https://icesat-2.gsfc.nasa.gov"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "expanded-prince",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "metadata": {
     "collapsed": false
    },
    "source": [
     "\n"
    ]
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
